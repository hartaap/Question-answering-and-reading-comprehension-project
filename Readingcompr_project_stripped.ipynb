{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Authors:** Alex THELANDER, Johanna ENGSNER, Aape HARTIKAINEN, Vladimir BLIZNIUKOV. \\\\\n",
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we have made a workflow on the eHoVy RACE reading-comprehension dataset from Hugging Face. We begin with data loading and **preliminary analysis** to understand the structure, vocabulary and class balance of the dataset.\n",
    "\n",
    "Next, we train and compare a variety of models:\n",
    "\n",
    "1. **Simple baselines** (TF-IDF + logistic regression)  \n",
    "2. **Sequence models** (LSTM-based classifier)  \n",
    "3. **Large language model (LLM) zero-shot/feature-based** approaches  \n",
    "4. **Fine-tuning an LLM** (flan T5) with Lora\n",
    "\n",
    "Finally, we explore a few **extensions**:\n",
    "\n",
    "- Applying our pipeline to **additional datasets** with the best fine-tuned ALBERT model\n",
    "- Building a **distractor-generation model** to automatically craft plausible but incorrect answer choices  \n",
    "\n",
    "> **Note:** For convenience, cells with all required `pip install` commands, imports and data-loading steps are spread throughout the notebook so you can rerun each section independently, but make sure to run the loading of data before any other cell, otherwise the notebook breaks.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading of Data - run before any other cell"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Colab setup: upgrade fsspec & friends, then install all deps\n",
    "!pip install --upgrade pip\n",
    "!pip install -U datasets\n",
    "!pip install --upgrade fsspec s3fs gensim numpy\n",
    "!pip install datasets transformers torch tqdm scipy scikit-learn"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Install the datasets library\n",
    "!pip install -q datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "race = load_dataset(\"ehovy/race\", \"all\")\n",
    "\n",
    "#inspect the available splits\n",
    "print(race)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Preliminary Analysis of the Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Length per split (number of datapoints):\\n train: {len(race['train'])},\\n test: {len(race['test'])},\\n validation: {len(race['validation'])}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Race dataset to pandas data frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "def flatten_race_batch(batch):\n",
    "    new_batch = {\n",
    "        \"article\": [],\n",
    "        \"question\": [],\n",
    "        \"options\": [],\n",
    "        \"answer\": []\n",
    "    }\n",
    "    for article, question, options, answer in zip( batch[\"article\"], batch[\"question\"], batch[\"options\"], batch[\"answer\"]):\n",
    "            new_batch[\"article\"].append(article)\n",
    "            new_batch[\"question\"].append(question)\n",
    "            new_batch[\"options\"].append(options)\n",
    "            new_batch[\"answer\"].append(answer)\n",
    "    return new_batch\n",
    "\n",
    "# Apply with batched=True\n",
    "flat_train = pd.DataFrame(race[\"train\"].map(flatten_race_batch, batched=True, remove_columns=race[\"train\"].column_names))\n",
    "flat_train[\"split\"] = \"train\"\n",
    "flat_val = pd.DataFrame(race[\"validation\"].map(flatten_race_batch, batched=True, remove_columns=race[\"validation\"].column_names))\n",
    "flat_val[\"split\"] = \"validation\"\n",
    "flat_test = pd.DataFrame(race[\"test\"].map(flatten_race_batch, batched=True, remove_columns=race[\"test\"].column_names))\n",
    "flat_test[\"split\"] = \"test\"\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_race = pd.concat([flat_train, flat_val, flat_test])\n",
    "\n",
    "letter_to_index = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "\n",
    "df_race[\"correct_answer\"] = df_race.apply(\n",
    "    lambda row: row[\"options\"][letter_to_index[row[\"answer\"]]],\n",
    "    axis=1\n",
    ")\n",
    "# map letter answer to a number\n",
    "df_race[\"answer_num\"] = df_race[\"answer\"].replace(letter_to_index)\n",
    "df_race.head()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question and Answer distributions per article"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# rpoup by article and split and count the number of questions\n",
    "article_question_counts = df_race.groupby(['article', 'split']).size().reset_index(name='question_count')\n",
    "\n",
    "def percent_hist(x, **kwargs):\n",
    "    # plot histogram normalized (density=True)\n",
    "    weights = (100 / len(x)) * pd.Series([1] * len(x))\n",
    "    plt.hist(x, bins=20, weights=weights, **kwargs)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel(\"Number of Questions\")\n",
    "    plt.ylabel(\"Percentage of Articles\")\n",
    "\n",
    "# create grid with split-wise histograms (train, test, validation)\n",
    "g = sns.FacetGrid(article_question_counts, col=\"split\", sharey=False, sharex=False)\n",
    "g.map(percent_hist, \"question_count\")\n",
    "g.fig.suptitle('Number of Questions per Article by Split', y=1.03)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:** There are articles that contain only one question, moreover, in both train and test the number of 3-question articles is the highest."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# filter articles with exactly one question\n",
    "articles_with_one_question = article_question_counts[article_question_counts['question_count'] == 1]\n",
    "\n",
    "# count the number of such articles per split\n",
    "articles_per_split_one_question = articles_with_one_question.groupby('split').size().reset_index(name='article_count')\n",
    "\n",
    "print(\"Number of articles with one question per split:\")\n",
    "articles_per_split_one_question"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# find the article with the maximum number of questions\n",
    "article_with_most_questions = article_question_counts.loc[article_question_counts['question_count'].idxmax()]\n",
    "\n",
    "print(\"Article with the most number of questions:\", article_with_most_questions[\"question_count\"],\"questions in\", article_with_most_questions[\"split\"])\n",
    "\n",
    "article_with_most_questions[\"article\"]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# find the article with the maximum count for answer 'A'\n",
    "# calculate the count of each answer choice ('A', 'B', 'C', 'D') for each article\n",
    "answer_counts_per_article = df_race.groupby('article')['answer'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "article_with_most_A = answer_counts_per_article['A'].idxmax()\n",
    "max_A_count = answer_counts_per_article['A'].max()\n",
    "\n",
    "print(f\"Article with the most number of answer 'A':\\n\")\n",
    "print(f\"Article content:\\n{article_with_most_A}\")\n",
    "print(f\"\\nNumber of 'A' answers in this article: {max_A_count}\")\n",
    "\n",
    "# optionally, display all answers for this article\n",
    "print(f\"\\nAnswer distribution for this article:\")\n",
    "print(answer_counts_per_article.loc[article_with_most_A].values)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def percent_hist(x, **kwargs):\n",
    "    # plot histogram normalized (density=True)\n",
    "    weights = (100 / len(x)) * pd.Series([1] * len(x))\n",
    "    plt.hist(x, bins=20, weights=weights, **kwargs)\n",
    "    plt.xlabel(\"Number of Questions\")\n",
    "    plt.ylabel(\"Percentage of Points\")\n",
    "\n",
    "# create grid with split-wise histograms\n",
    "g = sns.FacetGrid(df_race.sort_values([\"answer\"]), col=\"split\", sharey=False, sharex=False)\n",
    "g.map(percent_hist, \"answer\")\n",
    "g.fig.suptitle('Percentage of Answers per Split', y=1.03)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:** The distribution is almost equivalent in all splits, perhaps, in train the answer B is less common"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_race.groupby([\"split\", \"article\"]).agg(q_count=(\"question\", \"count\")).groupby(\"split\").agg(avg_question_per_article=(\"q_count\", \"mean\")).reset_index()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd\n",
    "\n",
    "def get_max_str(str_list):\n",
    "  return str_list[np.argmax([len(s) for s in str_list])]\n",
    "\n",
    "def plot_info(df_names, splits, unique_flag=False):\n",
    "    # seaborn style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    token_pattern = re.compile(r'\\b\\w+\\b')\n",
    "    if len(df_names) > 1:\n",
    "        df = []\n",
    "        for split in splits:\n",
    "            for ex in race[split]:\n",
    "                # lower‐case and split out words from df_names\n",
    "                text = \" \".join([ex[df_name] if type(ex[df_name]) == str else get_max_str(ex[df_name]) for df_name in df_names ])\n",
    "                df.append(text)\n",
    "    else:\n",
    "      df = []\n",
    "      for split in splits:\n",
    "        df += race[split][df_names[0]]\n",
    "    if unique_flag:\n",
    "      df = set(df)\n",
    "    tokens = [token_pattern.findall(a.lower()) for a in df]\n",
    "\n",
    "    # calculate statistics on tokenized data\n",
    "\n",
    "    lengths = np.array([len(toks) for toks in tokens])\n",
    "    vocab_sizes = np.array([len(set(toks)) for toks in tokens])\n",
    "    collection_vocab = {tok for toks in tokens for tok in toks}\n",
    "\n",
    "    # Compute percentiles and min/max\n",
    "    percentiles = np.percentile(lengths, [0, 25, 50, 75, 100])\n",
    "    labels = [\"Min\", \"25th\", \"Median\", \"75th\", \"Max\"]\n",
    "\n",
    "    # Create plt figure\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    title = ' + '.join([df_name.capitalize() for df_name in df_names])\n",
    "\n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(lengths, bins=50, kde=True, color='skyblue')\n",
    "    plt.title(f\"{title} Length Distribution (Histogram)\")\n",
    "    plt.xlabel(f\"{title} Length (# words)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    mean = np.round(lengths.mean(),1)\n",
    "    plt.axvline(np.round(lengths.mean(),1), color='red', linestyle='--')\n",
    "    plt.text(mean, plt.ylim()[1]*0.4, f'Avg Length: {mean}', color='red', rotation=90)\n",
    "\n",
    "    # Box Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=lengths, color='lightgreen')\n",
    "    plt.title(f\"{title} Length Distribution (Box Plot)\")\n",
    "    plt.xlabel(f\"{title} Length (# words)\")\n",
    "\n",
    "    # Add labels on the box plot\n",
    "    for p, label in zip(percentiles, labels):\n",
    "        plt.text(p, 0.02, f'{label}: {int(p)}', rotation=90, verticalalignment='bottom', color='darkblue')\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    stats = {\n",
    "    \"Metric\": [\n",
    "        \"Number of points\",\n",
    "        \"Collection vocab size\",\n",
    "        \"Average unique tokens per example\"\n",
    "    ],\n",
    "    \"Value\": [len(df),\n",
    "              len(collection_vocab),\\\n",
    "              np.round(vocab_sizes.mean(),1)]\n",
    "    }\n",
    "\n",
    "    display(Markdown(f\"## {title} info\"))\n",
    "    display(pd.DataFrame(stats))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lengths distribution of Article, Questions, Options\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# plot info about UNIQUE articles in all splits\n",
    "plot_info([\"article\"],[\"train\", \"validation\", \"test\"], True) # last parameter refers to uniquness"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_race.loc[df_race[\"article\"].str.findall(r'\\b\\w+\\b').str.len() < 10] # look at the articles that are less than 10 words"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:** These articles contain no significant information, yet they have valid questions + answers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plot_info([\"question\"],[\"train\", \"validation\", \"test\"], unique_flag=False)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_race.loc[df_race[\"question\"].str.findall(r'\\b\\w+\\b').str.len() == 0].shape"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_race.loc[df_race[\"question\"].str.findall(r'\\b\\w+\\b').str.len() < 3] # questions shortes than 3 words"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**If the question length is 1,** it is most likely to give the definition of the word like in the example above for 'Doctors_.', 'Mauritiusis _ .'\n",
    "\n",
    "**However, if the question length is 0,** it is less logical. Sometimes the question is inside the text:\n",
    "\n",
    "The best title of the text is _ . A. Various Opinions on Japan's Nuclear Disaster B. Japan's Disaster is Likely to Run out of Control C. America Feels Great Concern for Japan's Nuclear Crisis D. Japan's Disaster Throws Doubt on Nuclear Energy Industry\n",
    "\n",
    "**Sometimes the text itself makes no sense:**\n",
    "\n",
    "At the same time, children whose parents read to them, take them on outings and just generally pay attention to them are less likely to become bullies, said the report from the University of Washington. Researchers also found gaps in learning and understanding such things as social skills early in life makes it more difficult for children to relate with other children. Watching violence on television leads to aggressive behavior.\n",
    "\n",
    "*(??) - Continues with the following, which is not relevant at all.*\n",
    "\n",
    "You could soon be able to add your favorite perfume to your e-mails. UK net provider Telewest Broadband is testing a system to let people send e-mails over the Internet with sweet smell. It has developed a kind of hi-tech air freshener that plugs into a PC and sprays a smell linked to the message. Telewest says it could be used by supermarkets to attract people with the smell of fresh bread or by holiday companies seeking to stir up images of sun-kissed beachers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plot_info([\"article\", \"question\"],[\"train\"], True) # unique combinations of article + question (to look at the length)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_info([\"article\", \"question\", \"options\"],[\"train\"], True) # unique combinations of article + question + option (to look at the length)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check overlapping articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for duplicate articles across different splits\n",
    "# We group by article content and then check if a single article appears in more than one split.\n",
    "article_splits = df_race.groupby('article')['split'].nunique()\n",
    "\n",
    "# Filter articles that appear in more than one split\n",
    "duplicate_articles = article_splits[article_splits > 1]\n",
    "\n",
    "if duplicate_articles.empty:\n",
    "    print(\"No articles appear in multiple splits.\")\n",
    "else:\n",
    "    print(\"The following articles appear in multiple splits:\")\n",
    "    print(duplicate_articles)\n",
    "\n",
    "    # Optionally, display the information for one such article\n",
    "    example_article = duplicate_articles.index[0]\n",
    "    print(f\"\\nDetails for one duplicate article (first one found):\")\n",
    "    display(df_race[df_race['article'] == example_article])\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Top questions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Count question frequencies and print the top 20 most common\n",
    "from collections import Counter\n",
    "df_race_train = df_race[df_race[\"split\"]==\"train\"]\n",
    "questions = df_race_train[\"question\"].values\n",
    "question_counts = Counter(questions)\n",
    "\n",
    "# Get the 20 most common questions\n",
    "top20 = question_counts.most_common(20)\n",
    "\n",
    "print(\"Top 20 most common questions (with their counts):\")\n",
    "for question, count in top20:\n",
    "    print(f\"{count:5d} × {question}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_race[\"question_type\"] = df_race[\"question\"].apply(lambda q: \"to-continue\" if q.replace(\" \", \"\").endswith(\"_.\") else \"to-answer\")\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = Counter(df_race[\"question_type\"].values)\n",
    "plt.bar(counts.keys(), counts.values(), color='skyblue')\n",
    "plt.title(f'Question type distribution across datasets')\n",
    "plt.xlabel('Question type')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U spacy\n",
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count POS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load nlp model with unused components disabled\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\", \"lemmatizer\"])\n",
    "\n",
    "# get unique articles\n",
    "articles = df_race_train[\"article\"].unique()\n",
    "\n",
    "# use tqdm to track progress\n",
    "pos_counts = Counter()\n",
    "for doc in tqdm(nlp.pipe(articles, batch_size=32, n_process=1), total=len(articles), desc=\"Processing Articles\"):\n",
    "    pos_counts.update(token.pos_ for token in doc)\n",
    "\n",
    "# plot\n",
    "sorted_pairs = sorted(zip(pos_counts.keys(), pos_counts.values()), key=lambda x: x[1], reverse=True)\n",
    "sorted_labels, sorted_values = zip(*sorted_pairs)\n",
    "plt.bar(sorted_labels, sorted_values, color='skyblue')\n",
    "plt.title('Part of speech distribution in all splits')\n",
    "plt.xlabel('POS')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most of the terms are nouns, followed by verbs and pronouns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip uninstall torch -y\n",
    "# !pip install torch --upgrade"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from spacy.tokens import Span\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# collect all articles\n",
    "articles = df_race_train[\"article\"].unique()\n",
    "\n",
    "def show_article_with_most_pos(pos_tag):\n",
    "    \"\"\"Display the article with the most occurrences of a given POS tag.\"\"\"\n",
    "    max_count = -1\n",
    "    max_article = None\n",
    "    max_doc = None\n",
    "    # search for the exact tag and count\n",
    "    for entry, doc in zip(tqdm(articles, desc=f\"Searching for most {pos_tag}\"),\n",
    "                          nlp.pipe((a for a in articles), batch_size=32)):\n",
    "        # divide the count by the coefficient to get the per 100 words density of text\n",
    "        count = sum(1 for token in doc if token.pos_ == pos_tag)/(len(doc)//100) if len(doc) > 100 else 0\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_article = entry\n",
    "            max_doc = doc\n",
    "\n",
    "    # output summary\n",
    "    print(f\"\\n--- Most {pos_tag} article ---\")\n",
    "    print(\"POS count:\", max_count)\n",
    "\n",
    "    # lines to add coloring of extracted entities (actually, pos)\n",
    "    ents = [Span(max_doc, i, i+1, label=pos_tag) for i, token in enumerate(max_doc) if token.pos_ == pos_tag]\n",
    "    max_doc.set_ents(ents)\n",
    "\n",
    "    # display the colorings\n",
    "    colors = {pos_tag: \"rgba(144, 238, 144, 0.6)\"}  # light green with transparency\n",
    "    options = {\"ents\": [pos_tag], \"colors\": colors}\n",
    "    html = displacy.render(max_doc, style=\"ent\", options=options, jupyter=False)\n",
    "    display(HTML(html))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Highest POS density articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "show_article_with_most_pos(\"ADJ\")   # adjectives"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_article_with_most_pos(\"VERB\")  # verbs"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_article_with_most_pos(\"NOUN\")  # nouns"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Cloud"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Combine all article texts into one string\n",
    "all_text = \" \".join(df_race_train[\"article\"].unique())\n",
    "\n",
    "tokens = word_tokenize(all_text.lower())\n",
    "\n",
    "# Remove stopwords and punctuation\n",
    "filtered_tokens = [word for word in tokens\n",
    "                   if word not in stopwords.words('english') and word.isalpha()]\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Greens').generate(all_text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Articles\", fontsize=20)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the word cloud of train articles the most popular words are people, one, said, make, student, world, children — very logical for a high/middle school selection."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "import re\n",
    "\n",
    "def cluster_articles(texts, cleaning_function, n_clusters=10):\n",
    "    # apply cleaning to the articles\n",
    "    texts_cleaned = [cleaning_function(text) for text in texts]\n",
    "\n",
    "    # TF–IDF vectorization (limit features for speed)\n",
    "    tfidf = TfidfVectorizer(max_features=10_000, max_df = 0.8, min_df=5, stop_words=\"english\")\n",
    "    X_tfidf = tfidf.fit_transform(texts_cleaned)\n",
    "    vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "    # K-Means clustering (for now 10 clusters)\n",
    "    n_clusters = 10\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = km.fit(X_tfidf)\n",
    "\n",
    "    print(\"Top terms per cluster:\")\n",
    "    vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "    for i in range(km.n_clusters):\n",
    "        centroid = km.cluster_centers_[i]\n",
    "        sorted_terms = centroid.argsort()[::-1]\n",
    "        print(f\"Cluster {i}:\\t{[vocab[j] for j in sorted_terms[:10]]}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "texts = df_race_train[\"article\"].unique()\n",
    "# Original clustering for train\n",
    "def clean_text_orig(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    return text.lower()\n",
    "\n",
    "cluster_articles(texts, clean_text_orig)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cluster 0:\tFood/nature \\\\\n",
    "Cluster 1:\tTechnology \\\\\n",
    "Cluster 2:\tSchool \\\\\n",
    "Cluster 3:\tLeisure \\\\\n",
    "Cluster 4:\t?? \\\\\n",
    "Cluster 5:\t?? \\\\\n",
    "Cluster 6:\tFamily \\\\\n",
    "Cluster 7:\t?? \\\\\n",
    "Cluster 8:\tHealth \\\\\n",
    "Cluster 9:\tLanguages \\\\\n",
    "\n",
    "**Issues:** languages, language; food, foods; learning, learn; - are present in the clusters, however, give no new info => apply stemming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download('punkt_tab')\n",
    "# stemmed clustering for train\n",
    "\n",
    "def clean_text_stemming(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(word) for word in tokens] # apply stemming\n",
    "    return ' '.join(stemmed)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cluster_articles(texts, clean_text_stemming)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "More defined (logical) clusters:\n",
    "\n",
    "Cluster 0:\tNature (more defined) \\\\\n",
    "Cluster 1:\tLanguages \\\\\n",
    "Cluster 2:\tLeisure \\\\\n",
    "Cluster 3:\t['said', 'man', 'mother', 'day', 'father', 'ask', 'boy', 'look', 'went', 'old'] ?? \\\\\n",
    "Cluster 4:\tEducation \\\\\n",
    "Cluster 5:\tSleeping \\\\\n",
    "Cluster 6:\t['peopl', 'year', 'citi', 'world', 'said', 'new', 'use', 'time', 'china', 'work'] ?? \\\\\n",
    "Cluster 7:\tMusic \\\\\n",
    "Cluster 8:\tFamily \\\\\n",
    "Cluster 9:\tFood \\\\\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "texts = df_race[df_race[\"split\"]==\"test\"][\"article\"].unique()\n",
    "cluster_articles(texts, clean_text_stemming)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Actually, a bit different clusters on test set\n",
    "\n",
    "Cluster 0:\tLearning \\\\\n",
    "Cluster 1:\tSpace \\\\\n",
    "Cluster 2:\t['said', 'day', 'tom', 'friend', 'went', 'mr', 'jack', 'veri', 'taxi', 'hotel'] ?? \\\\\n",
    "Cluster 3:\t['man', 'said', 'time', 'friend', 'thing', 'say', 'life', 'ask', 'day', 'make'] ?? \\\\\n",
    "Cluster 4:\tWater \\\\\n",
    "Cluster 5:\t['peopl', 'world', 'food', 'new', 'year', 'use', 'china', 'like', 'mani', 'citi'] ?? \\\\\n",
    "Cluster 6:\tSports \\\\\n",
    "Cluster 7:\tFlora/Fauna \\\\\n",
    "Cluster 8:\tEducation \\\\\n",
    "Cluster 9:\tFamily \\\\\n",
    "\n",
    "Clusters are a bit different"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Searching for number of clusters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = []\n",
    "labels_list = []\n",
    "k_range = range(2, 30)\n",
    "\n",
    "# get texts\n",
    "texts = df_race_train[\"article\"].unique()\n",
    "# clean the texts\n",
    "texts_cleaned = [clean_text_stemming(text) for text in texts]\n",
    "\n",
    "# TF–IDF vectorization (limit features for speed)\n",
    "tfidf = TfidfVectorizer(max_features=10_000, max_df = 0.8, min_df=5, stop_words=\"english\")\n",
    "X_tfidf = tfidf.fit_transform(texts_cleaned)\n",
    "# get vocab\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "for k in k_range:\n",
    "    print(f\"Fitting MiniBatchKMeans with k = {k}\")\n",
    "    km = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1024) # mini batch faster training\n",
    "    labels = km.fit_predict(X_tfidf)\n",
    "    score = silhouette_score(X_tfidf, labels, sample_size=5000, random_state=42)\n",
    "    scores.append((k, score))\n",
    "    labels_list.append((km, labels))\n",
    "\n",
    "# choose the best K\n",
    "best_k, best_score = max(scores, key=lambda x: x[1])\n",
    "print(f\"\\n Best number of clusters: {best_k} (silhouette score = {best_score:.3f})\")\n",
    "\n",
    "# use best model\n",
    "best_km, best_labels = labels_list[best_k - 2]\n",
    "\n",
    "# print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "for i in range(best_k):\n",
    "    centroid = best_km.cluster_centers_[i]\n",
    "    sorted_terms = centroid.argsort()[::-1][:10]\n",
    "    top_terms = [vocab[j] for j in sorted_terms]\n",
    "    print(f\"Cluster {i}:\\t{top_terms}\")\n",
    "\n",
    "ks, s_scores = zip(*scores)\n",
    "plt.plot(ks, s_scores, marker='o')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Scores vs. k\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overall, the scores are really low, which does not say anything good about the clustering. Therefore, we can say that 28 clusters is the best option, however, due to the point above it makes no difference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dimensionality Reduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# reduced dimentions of document vectors (SVD)\n",
    "svd = TruncatedSVD(3)\n",
    "reduced_data = svd.fit_transform(X_tfidf)\n",
    "\n",
    "[x,y,z] = np.transpose(reduced_data)\n",
    "[x,y,z]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, c=km.labels_, marker='.');\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# reduced dimentions of document vectors (t-SNE)\n",
    "texts = df_race_train[\"article\"].unique()\n",
    "\n",
    "texts_cleaned = [clean_text_stemming(text) for text in texts]\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10_000, max_df = 0.8, min_df=5, stop_words=\"english\")\n",
    "X_tfidf = tfidf.fit_transform(texts_cleaned)\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42) # call tsne\n",
    "tsne_results = tsne.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "[x, y, z] = np.transpose(tsne_results)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, c=km.labels_, marker='.')\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Latent Dirichlet Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "pip install --upgrade pyLDAvis"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.lda_model  # updated import\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load train and test sets\n",
    "texts_train = df_race_train[\"article\"].unique()\n",
    "texts_test = df_race[df_race[\"split\"]==\"test\"][\"article\"].unique()\n",
    "\n",
    "\n",
    "# clean the articles (stemming from previous step)\n",
    "cleaned_texts_train = [clean_text_stemming(text) for text in texts_train]\n",
    "\n",
    "# use count vectoriser for LDA (generative model)\n",
    "vectorizer = CountVectorizer(stop_words='english',min_df=5,max_df=.5)\n",
    "vector_documents_train = vectorizer.fit_transform(cleaned_texts_train)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# fit LDA\n",
    "lda = LatentDirichletAllocation(n_components=30, verbose=1, learning_method='online', max_iter=30)\n",
    "lda.fit(vector_documents_train)\n",
    "\n",
    "# evaluating generalization of lda on test split\n",
    "cleaned_texts_test = [clean_text_stemming(text) for text in texts_test]\n",
    "vector_documents_test = vectorizer.transform(cleaned_texts_test)\n",
    "print(lda.perplexity(vector_documents_test))\n",
    "\n",
    "for i in range(len(lda.components_)):\n",
    "    sorted_terms = lda.components_[i].argsort()[::-1]\n",
    "    concatenated_terms = '[' + ', '.join(vocab[i] for i in sorted_terms[:10]) + ']'\n",
    "    print (f'Topic {i + 1}:\\t', concatenated_terms)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# LDA for 20 topics\n",
    "lda = LatentDirichletAllocation(n_components=30, verbose=1, learning_method='online', max_iter=30)\n",
    "lda.fit(vector_documents_train)\n",
    "\n",
    "# evaluating generalization of lda on test split\n",
    "cleaned_texts_test = [clean_text_stemming(text) for text in texts_test]\n",
    "vector_documents_test = vectorizer.transform(cleaned_texts_test)\n",
    "print(lda.perplexity(vector_documents_test))\n",
    "\n",
    "for i in range(len(lda.components_)):\n",
    "    sorted_terms = lda.components_[i].argsort()[::-1]\n",
    "    concatenated_terms = '[' + ', '.join(vocab[i] for i in sorted_terms[:10]) + ']'\n",
    "    print (f'Topic {i + 1}:\\t', concatenated_terms)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# topic plot for lda with 30 topics\n",
    "num_words = 10\n",
    "cols = 5\n",
    "rows = int(len(lda.components_)/5)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, 4 * rows), sharex=True)\n",
    "axes = axes.flatten()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_terms_index = topic.argsort()[:-num_words - 1:-1]\n",
    "    top_terms = [vocab[i] for i in top_terms_index]\n",
    "    weights = topic[top_terms_index]\n",
    "    ax = axes[topic_idx]\n",
    "    ax.barh(top_terms, weights, height=0.7)\n",
    "    ax.set_title(f'Topic {topic_idx +1}',fontdict={'fontsize': 15})\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "doc = df_race_train.iloc[60][\"article\"] # get a random article\n",
    "print(doc)\n",
    "\n",
    "doc_vec = vectorizer.transform([doc])[0] # get document vector\n",
    "topic_vec = lda.transform(doc_vec)[0] # get topic representation\n",
    "\n",
    "print(\"Topic vector for article: \\n\",topic_vec) # print the topic vector for the article\n",
    "\n",
    "# get top-10 topics for the document\n",
    "sorted_topics = topic_vec.argsort()[::-1]\n",
    "sorted_prevalence = sorted(topic_vec)[::-1]\n",
    "\n",
    "for i in range(10):\n",
    "    if sorted_prevalence[i] < .01: break\n",
    "    topic = sorted_topics[i]\n",
    "    sorted_terms = np.flip(lda.components_[topic].argsort())\n",
    "    print (f'{100 * sorted_prevalence[i]:.1f}% Topic {topic}: {\" \".join(vocab[i] for i in sorted_terms[:10])}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# plot the documents in the test based on the most relevant topic\n",
    "tsne_embedding = TSNE(n_components=3).fit_transform(lda.transform(vector_documents_test))\n",
    "tsne_embedding.shape\n",
    "\n",
    "freq_topic = [topic_vec.argsort()[-1] for topic_vec in lda.transform(vector_documents_test)]\n",
    "\n",
    "[x, y, z] = np.transpose(tsne_embedding)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, c=LabelEncoder().fit_transform(freq_topic), marker='.');"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA) Summary\n",
    "\n",
    "### Dataset Splits\n",
    "- **Train**: 87,866 examples  \n",
    "- **Validation**: 4,887 examples  \n",
    "- **Test**: 4,934 examples\n",
    "\n",
    "- **Only** 27931 unique articles\n",
    "\n",
    "### Question Distribution per Passage\n",
    "- Distribution is similar across splits.\n",
    "- **Train/Test**: Most articles have **3 questions**.\n",
    "- **Validation**: Most common is **4 questions**.\n",
    "- **645** articles have **only 1 question**.\n",
    "- **1 article** contains the **maximum of 7 questions**.\n",
    "\n",
    "### Answer Options (A, B, C, D)\n",
    "- Distribution of options is almost equal across splits.\n",
    "- Slight skew: **Option B** is **slightly less common** in the train set.\n",
    "- On average, **3.5 options** per question.\n",
    "\n",
    "### Article Statistics\n",
    "- **Average length**: 287 words  \n",
    "- **Shortest article**: 2-3 words (8 rows)\n",
    "- **Longest article**: 1188 words  \n",
    "- Note: Some articles (e.g., *Study Center Courses*, *Passage 1*) lack meaningful content but still contain valid questions.\n",
    "\n",
    "### Question Statistics\n",
    "- **Average question length**: 9.7 words  \n",
    "- **Shortest**: `0` words (18 rows) — example: `\".\"` (either question in the text or not logical)\n",
    "- **Longest**: 65 words  \n",
    "- Question formats:\n",
    "  - **Majority**: End with **`?`** (to-answer)\n",
    "  - **High-portion**: End with **`_`** (to-fill)\n",
    "\n",
    "### Top Question Types (frequency)\n",
    "- **349×** Which of the following is TRUE according to the passage?  \n",
    "- **282×** What can we learn from the passage?  \n",
    "- **263×** Which of the following is TRUE?\n",
    "\n",
    "### Part-of-Speech (POS) Analysis\n",
    "- Most frequent: **Nouns**, followed by **verbs** and **pronouns**\n",
    "- High-density articles in **nouns**, **verbs**, **adjectives** highlighted\n",
    "\n",
    "### Word Cloud (Train Set)\n",
    "- Most frequent words:  \n",
    "  `people, one, said, make, student, world, children`  \n",
    "  → Common and appropriate for a school-level dataset\n",
    "\n",
    "### Text Clustering\n",
    "- Tried **KMeans with 10 clusters**\n",
    "- **WordTokenization**: Clusters are meaningful but show morphological variation (e.g., *language* vs *languages*)\n",
    "- **Stemming**: Improves cluster coherence, removes word variants\n",
    "- **MiniBatchKMeans**:\n",
    "  - Best score at **28 clusters**\n",
    "  - All silhouittes scores are low (< 0.007), suggesting poor clustering performance\n",
    "\n",
    "### Dimensionality Reduction\n",
    "- **Truncated SVD (3D)**: Not informative\n",
    "- **t-SNE (3D)**: A bit less spread allocation\n",
    "\n",
    "### Latent Dirichlet Analysis\n",
    "\n",
    "- Used train set documents\n",
    "- Extracted 30 topic\n",
    "- Perprexity on test is 4096 for 30 topics and 4133 for 20 topics\n",
    "- Visualised test in 3d using t-SNE and the most relevant topic\n",
    "\n",
    "### Additional Notes\n",
    "- **No article appears in more than one split**\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ========================================================================================================"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 - Training models\n",
    "\n",
    "## Simple Models - using simple models to baseline performance\n",
    "\n",
    "## LSTM - using LSTM with simple tokenizer to baseline performance additionally\n",
    "\n",
    "## LLM - using falcon on the dataset, comparing 0, 1, 5 shot performance\n",
    "\n",
    "## Fine-tuning of LLM - fine-tuning google flan t5 small to evaluate performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Simple Models on the Dataset\n",
    "* Logistic regression (with different penalties) with TF-IDF Vectorizer\n",
    "* Dot score with embeddings\n",
    "* Logistic regression (with different penalties) with embeddings\n",
    "* XGBoost with embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Load RACE (middle + high school splits)\n",
    "data = race\n",
    "train_raw = data[\"train\"]\n",
    "val_raw   = data[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sample has only one option, and label 0/1 for correct/incorrect\n",
    "def flatten_race(raw):\n",
    "    texts, labels = [], []\n",
    "    A = ord(\"A\")\n",
    "    for ex in raw:\n",
    "        passage   = ex[\"article\"]\n",
    "        question = ex[\"question\"]\n",
    "        options   = ex[\"options\"]\n",
    "        answer   = ex[\"answer\"]  #A/B/C/D\n",
    "        i = 0\n",
    "        for opt in (options): #Loop through options, creating one text for each option\n",
    "            # combine passage, question, and one choice\n",
    "            txt = passage + \" \" + question + \" \" + opt\n",
    "            texts.append(txt)\n",
    "            # label = 1 if this option matches the right answer\n",
    "            correct = (chr(A + i) == answer)\n",
    "            labels.append(int(correct))\n",
    "            i+=1\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "train_texts, train_labels = flatten_race(train_raw)\n",
    "val_texts,   val_labels   = flatten_race(val_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize with TF–IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2),max_df=0.95, min_df=3)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_val   = vectorizer.transform(val_texts)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val   = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with TF-IDF, using different penalties\n",
    "penalties = [None, \"l1\", \"l2\"]\n",
    "\n",
    "for pen in penalties:\n",
    "    if pen == \"l1\":\n",
    "        clf = LogisticRegression(solver=\"liblinear\",penalty=pen)\n",
    "    else:\n",
    "        clf = LogisticRegression(penalty=pen)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #for each question, pick the choice with highest predicted P(label=1), and compare to the true correct index.\n",
    "    probs = clf.predict_proba(X_val)[:, 1]  # P(label=1)\n",
    "\n",
    "    preds, trues = [], []\n",
    "    start = 0\n",
    "    for last_opt in range(4,len(probs),4):\n",
    "        p      = probs[start:last_opt] #Model output\n",
    "        lbls   = y_val[start:last_opt] #labels\n",
    "        pred_i = np.argmax(p)\n",
    "        true_i = np.argmax(lbls)   #Find right answer\n",
    "        preds.append(pred_i)\n",
    "        trues.append(true_i)\n",
    "        start = last_opt+1\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(\"TF-IDF + LogisticRegression (using penalty: \" + str(pen) + f\"). Validation accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten into (text [containing passage and question], opts, label) triples for embeddings\n",
    "def flatten_race_embd(raw):\n",
    "    texts, opts, labels = [], [], []\n",
    "    A = ord(\"A\")\n",
    "    for ex in raw:\n",
    "        passage   = ex[\"article\"]\n",
    "        question = ex[\"question\"]\n",
    "        options   = ex[\"options\"]\n",
    "        answer   = ex[\"answer\"]  #A/B/C/D\n",
    "        i = 0\n",
    "        txt = passage + \" \" + question\n",
    "        texts.append(txt)\n",
    "        for opt in (options): #Loop through options, creating one text for each option\n",
    "            opts.append(opt)\n",
    "            # label = 1 if this option matches the right answer\n",
    "            correct = (chr(A + i) == answer)\n",
    "            labels.append(int(correct))\n",
    "            i+=1\n",
    "    return texts, opts, labels\n",
    "\n",
    "train_texts, train_opts, train_labels = flatten_race_embd(train_raw)\n",
    "val_texts,   val_opts, val_labels   = flatten_race_embd(val_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch with check print\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the texts and options\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer , util\n",
    "\n",
    "embdr = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\n",
    "\n",
    "train_txts_embd = embdr.encode(train_texts,show_progress_bar=True,device='cuda')\n",
    "train_opts_embd = embdr.encode(train_opts,show_progress_bar=True,device='cuda')\n",
    "\n",
    "val_txts_embd = embdr.encode(val_texts,show_progress_bar=True,device='cuda')\n",
    "val_opts_embd = embdr.encode(val_opts,show_progress_bar=True,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super simple classification based on dot score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def embedding_clf(passage_question, options):\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = util.dot_score(passage_question, options)[0].tolist()\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make labels that work for the embeddings\n",
    "new_labels_train = []\n",
    "for i in range(0,len(train_labels),4):\n",
    "    new_labels_train.append(np.argmax(train_labels[i:i+4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run embedding classifier based on dot score\n",
    "prev = 0\n",
    "preds = []\n",
    "#This is super ugly, sorry\n",
    "for train_embd in train_txts_embd:\n",
    "    curr = prev+4\n",
    "    options = train_opts_embd[prev:curr]\n",
    "    prev = curr\n",
    "    preds.append(embedding_clf(train_embd,options))\n",
    "\n",
    "acc = accuracy_score(new_labels_train, preds)\n",
    "print(f\"Embedding classifier. Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformat the data for logistic regression, want each input to be passage+question+option\n",
    "#Simple implementation - could probably be done easier?\n",
    "train_embd_logreg = []\n",
    "start = 0\n",
    "for train_embd in train_txts_embd:\n",
    "    for i in range (start,start+4):\n",
    "        concatenated = np.concatenate((train_embd,train_opts_embd[i]), axis=0)\n",
    "        train_embd_logreg.append(concatenated)\n",
    "    start +=4\n",
    "train_embd_logreg = np.stack(train_embd_logreg)\n",
    "print(train_embd_logreg.shape)\n",
    "\n",
    "val_embd_logreg = []\n",
    "start = 0\n",
    "for val_embd in val_txts_embd:\n",
    "    for i in range (start,start+4):\n",
    "        concatenated = np.concatenate((val_embd,val_opts_embd[i]), axis=0)\n",
    "        val_embd_logreg.append(concatenated)\n",
    "    start +=4\n",
    "val_embd_logreg = np.stack(val_embd_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log regression with embeddings\n",
    "penalties = [None, \"l1\", \"l2\"]\n",
    "\n",
    "for pen in penalties:\n",
    "    if pen == \"l1\":\n",
    "        clf = LogisticRegression(solver=\"liblinear\",penalty=pen)\n",
    "    else:\n",
    "        clf = LogisticRegression(penalty=pen)\n",
    "\n",
    "    clf.fit(train_embd_logreg, train_labels)\n",
    "\n",
    "    #evaluate: for each question (grouped by qid), pick the choice with highest predicted P(label=1), and compare to the true correct index.\n",
    "    probs = clf.predict_proba(val_embd_logreg)[:, 1]  # P(label=1)\n",
    "\n",
    "    preds, trues = [], []\n",
    "    start = 0\n",
    "    for last_opt in range(4,len(probs),4):\n",
    "        p      = probs[start:last_opt] #Model output\n",
    "        lbls   = val_labels[start:last_opt] #labels\n",
    "        pred_i = np.argmax(p)\n",
    "        true_i = np.argmax(lbls)   #Find right answer\n",
    "        preds.append(pred_i)\n",
    "        trues.append(true_i)\n",
    "        start = last_opt+1\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(\"Embeddings + LogisticRegression (using penalty: \" + str(pen) + f\"). Validation accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings with XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf_xgb = XGBClassifier(eval_metric='logloss')\n",
    "clf_xgb.fit(train_embd_logreg, train_labels)\n",
    "probs = clf_xgb.predict_proba(val_embd_logreg)[:, 1]\n",
    "preds, trues = [], []\n",
    "start = 0\n",
    "for last_opt in range(4, len(probs), 4):\n",
    "    p = probs[start:last_opt]\n",
    "    lbls = val_labels[start:last_opt]\n",
    "    pred_i = np.argmax(p)\n",
    "    true_i = np.argmax(lbls)\n",
    "    preds.append(pred_i)\n",
    "    trues.append(true_i)\n",
    "    start = last_opt + 1\n",
    "\n",
    "# Calculate and print accuracy\n",
    "acc = accuracy_score(trues, preds)\n",
    "print(f\"Embeddings + XGBoost (default). Validation accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result Simple Models\n",
    "**Validation Accuracies by Model**  \n",
    "- TF-IDF + LogisticRegression (penalty=None): 0.3880  \n",
    "- TF-IDF + LogisticRegression (penalty=l1): 0.4099  \n",
    "- TF-IDF + LogisticRegression (penalty=l2): 0.3880  \n",
    "- Embedding classifier (nearest‐centroid style): 0.3626  \n",
    "- Embeddings + LogisticRegression (penalty=None): 0.4095  \n",
    "- Embeddings + LogisticRegression (penalty=l1): 0.4112  \n",
    "- Embeddings + LogisticRegression (penalty=l2): 0.4147  \n",
    "- Embeddings + XGBoost (default settings): 0.4187  \n",
    "\n",
    "**Discussion**  \n",
    "1. **Feature comparison**: Embedding-based features outperform raw TF-IDF slightly, especially when paired with regularized logistic regression (best TF-IDF: 0.4099 vs. best embeddings: 0.4147).  \n",
    "2. **Classifier impact**: Adding a lightweight classifier to embeddings makes a big jump from the raw “embedding classifier” baseline (0.3626 → 0.4095+).  \n",
    "3. **Model complexity**: Moving from logistic regression to a more flexible learner (XGBoost) gives the highest accuracy (0.4187), but gains are very low, suggesting these models, as expected don't work well on this type of task.  \n",
    "\n",
    "**Conclusion**  \n",
    "Simple linear and tree-based models on TF-IDF or embedding features top out around 0.42 accuracy on RACE with little to none change in accuracy. This indicating that to increase performance further you would need deeper and more complex architectures.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM\n",
    "* To create a simple baseline before turning to LLMs, LSTM was used.\n",
    "* Together with a simple tokenizer from scratch an LSTM was created and trained on the data.\n",
    "* Its accuracy was then evaluated to measure a baseline accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "#Tokenize\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"<\\w+>|[\\w']+\", text.lower())\n",
    "\n",
    "#Load dataset\n",
    "data = load_dataset(\"ehovy/race\", \"all\")\n",
    "train_hf = data[\"train\"]#.select(range(10000))\n",
    "val_hf   = data[\"validation\"]#.select(range(100))\n",
    "\n",
    "#Map answers to 0–3\n",
    "label_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "train_hf = train_hf.map(lambda x: {\"label\": label_map[x[\"answer\"]]})\n",
    "val_hf   = val_hf.map(lambda x: {\"label\": label_map[x[\"answer\"]]})\n",
    "\n",
    "#Build vocab\n",
    "counter = Counter()\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<article>\", \"<question>\", \"<option>\"]\n",
    "vocab = {tok: i for i, tok in enumerate(special_tokens)}\n",
    "\n",
    "for ex in train_hf:\n",
    "    text = f\"<article> {ex['article']} <question> {ex['question']} \" + \\\n",
    "           \" \".join(f\"<option> {opt}\" for opt in ex[\"options\"])\n",
    "    counter.update(tokenize(text))\n",
    "\n",
    "for w, freq in counter.items():\n",
    "    if w not in vocab and freq > 1:\n",
    "        vocab[w] = len(vocab)\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class RACEDataset(Dataset):\n",
    "    def __init__(self, hf_ds, vocab):\n",
    "        self.data = hf_ds\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        article, question, opts = ex[\"article\"], ex[\"question\"], ex[\"options\"]\n",
    "        seqs = []\n",
    "        for opt in opts:\n",
    "            txt = f\"<article> {article} <question> {question} <option> {opt}\"\n",
    "            ids = [self.vocab.get(tok, self.vocab[\"<unk>\"]) for tok in tokenize(txt)]\n",
    "            seqs.append(torch.tensor(ids, dtype=torch.long))\n",
    "        return seqs, torch.tensor(ex[\"label\"], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labels = zip(*batch)     # seqs: list of [4×tensors], labels: list of scalars\n",
    "    B = len(seqs)\n",
    "    C = len(seqs[0])               # =4\n",
    "\n",
    "    # 1) flatten all B×C sequences into one list\n",
    "    flat = [s for sample in seqs for s in sample]\n",
    "    lengths = torch.tensor([len(s) for s in flat], dtype=torch.long)\n",
    "\n",
    "    # 2) pad them together to the max length\n",
    "    padded = pad_sequence(flat, batch_first=True, padding_value=vocab[\"<pad>\"])  # [B*C, L]\n",
    "\n",
    "    # 3) reshape back to [B, C, L]\n",
    "    L = padded.size(1)\n",
    "    input_ids = padded.view(B, C, L)\n",
    "    lengths   = lengths.view(B, C)\n",
    "    labels    = torch.stack(labels)\n",
    "\n",
    "    return input_ids, lengths, labels\n",
    "\n",
    "# LSTM model\n",
    "class RACELSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm  = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.lin   = nn.Linear(hid_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        # input_ids: [B,4,L], lengths: [B,4]\n",
    "        B, C, L = input_ids.size()\n",
    "        flat_ids  = input_ids.view(B*C, L)            # [B*C, L]\n",
    "        flat_lens = lengths.view(-1).cpu()            # [B*C]\n",
    "\n",
    "        emb = self.embed(flat_ids)                    # [B*C, L, E]\n",
    "        packed = pack_padded_sequence(emb, flat_lens,\n",
    "                                      batch_first=True,\n",
    "                                      enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)               # h_n: [1, B*C, H]\n",
    "        h = h_n.squeeze(0)                            # [B*C, H]\n",
    "\n",
    "        logits = self.lin(h).view(B, C)               # [B,4]\n",
    "        return logits\n",
    "\n",
    "#Check if cuda to run faster\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = RACEDataset(train_hf, vocab)\n",
    "val_ds   = RACEDataset(val_hf, vocab)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "model = RACELSTM(len(vocab), emb_dim=200, hid_dim=128, pad_idx=vocab[\"<pad>\"]).to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit  = nn.CrossEntropyLoss()\n",
    "\n",
    "patience, best_acc, counter = 3, 0.0, 0\n",
    "\n",
    "#Train model\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for input_ids, lengths, labels in train_loader:\n",
    "        input_ids, lengths, labels = input_ids.to(device), lengths.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(model(input_ids, lengths), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Check performance on validation set\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, lengths, labels in val_loader:\n",
    "            input_ids, lengths, labels = input_ids.to(device), lengths.to(device), labels.to(device)\n",
    "            preds = model(input_ids, lengths).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} — val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best-RACE-lstm.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"No improvement for {patience} epochs, stopping.\")\n",
    "            break\n",
    "\n",
    "# reload best\n",
    "model.load_state_dict(torch.load(\"best-RACE-lstm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running model on test set\n",
    "test_hf = data[\"test\"]#.select(range(100))\n",
    "test_hf = test_hf.map(lambda x: {\"label\": label_map[x[\"answer\"]]})\n",
    "test_ds = RACEDataset(test_hf, vocab)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best-RACE-lstm.pth\"))\n",
    "model.to(device).eval()\n",
    "\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for input_ids, lengths, labels in test_loader:\n",
    "        input_ids, lengths, labels = (\n",
    "            input_ids.to(device),\n",
    "            lengths.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "        logits = model(input_ids, lengths)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusions LSTM\n",
    "\n",
    "* Results quite similar to the simple models above\n",
    "* As it was trained from scratch with a small vocabulary and no pretrained embeddings.\n",
    "* Since the input representation is also relying on basic tokenizer and ignores contextual embeddings, the model behaves quite similarly to simpler baselines like logistic regression + TF-IDF used above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Intro\n",
    "\n",
    "BERT - bidirectional encoder representation of transformer. Training consists of fine-tuning the last layer or the classifier.\n",
    "\n",
    "**Input format:**\n",
    "\n",
    "- Each multiple-choice question is paired with its possible answers.\n",
    "- For each (question, choice) pair, a separate input is created:\n",
    "[CLS] article [SEP] question + choice [SEP] depending on dataset.\n",
    "\n",
    "**Tokenization:**\n",
    "- All inputs are tokenized using BERT's tokenizer.\n",
    "- Truncated or padded to fit within 512 tokens.\n",
    "\n",
    "**Classification layer:**\n",
    "\n",
    " - A linear layer (with or without softmax) is applied to the [CLS] embeddings.\n",
    " - Outputs a single score per (question, choice) pair.\n",
    "\n",
    "**Prediction:**\n",
    "- Scores for all choices are compared.\n",
    "- The choice with the highest score is selected as the model's prediction.\n",
    "\n",
    "**Advantages of BERT:**\n",
    "\n",
    "- Broad understanding of a language\n",
    "- Creates rich representations of words within the context\n",
    "- Has bidirectional context"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = load_dataset(\"race\", \"all\", split=\"train\")\n",
    "validation_dataset = load_dataset(\"race\", \"all\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"race\", \"all\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_newlines = lambda example: {\n",
    "    'article': re.sub(r'\\n+', '\\n', example['article']).strip()\n",
    "}\n",
    "train_dataset = train_dataset.map(normalize_newlines)\n",
    "validation_dataset = validation_dataset.map(normalize_newlines)\n",
    "test_dataset = test_dataset.map(normalize_newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "# tokenize the input sequences\n",
    "def preprocess(example, is_test=False):\n",
    "    choices_inputs = [\n",
    "        tokenizer.encode_plus(\n",
    "            example[\"article\"],\n",
    "            example[\"question\"] + \" \" + choice,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        for choice in example[\"options\"]\n",
    "    ]\n",
    "\n",
    "    # Convert list of dicts to dict of lists\n",
    "    input_ids = [ci[\"input_ids\"][0] for ci in choices_inputs]\n",
    "    attention_mask = [ci[\"attention_mask\"][0] for ci in choices_inputs]\n",
    "    token_type_ids = [ci[\"token_type_ids\"][0] for ci in choices_inputs]\n",
    "\n",
    "    result = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }\n",
    "\n",
    "    if not is_test:\n",
    "        result[\"labels\"] = letter_to_index[example[\"answer\"]]\n",
    "\n",
    "    return result\n",
    "\n",
    "train_enc_dataset = train_dataset.map(preprocess, fn_kwargs={\"is_test\": False})\n",
    "validation_enc_dataset = validation_dataset.map(preprocess, fn_kwargs={\"is_test\": False})"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"a1c5728b84c8114f556c894e4d564f9eb88b537d\"\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertForMultipleChoice, Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    warmup_ratio=0.05,\n",
    "    fp16=True, # used for faster training (mixd precision training)\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_enc_dataset,\n",
    "    eval_dataset=validation_enc_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "trainer.train()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"/content/drive/My Drive/johanna/results\")\n",
    "tokenizer.save_pretrained(\"/content/drive/My Drive/johanna/results\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "def preprocess(example, is_test=False):\n",
    "    # First, check if any of the 4 choices would exceed the max token limit\n",
    "    for choice in example[\"options\"]:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            example[\"article\"],\n",
    "            example[\"question\"] + \" \" + choice,\n",
    "            truncation=False,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        if len(encoded[\"input_ids\"]) > 512:\n",
    "            return None  # Skip this example entirely\n",
    "\n",
    "    # If all options are within limit, proceed with usual preprocessing\n",
    "    choices_inputs = [\n",
    "        tokenizer.encode_plus(\n",
    "            example[\"article\"],\n",
    "            example[\"question\"] + \" \" + choice,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        for choice in example[\"options\"]\n",
    "    ]\n",
    "\n",
    "    input_ids = [ci[\"input_ids\"][0] for ci in choices_inputs]\n",
    "    attention_mask = [ci[\"attention_mask\"][0] for ci in choices_inputs]\n",
    "    token_type_ids = [ci[\"token_type_ids\"][0] for ci in choices_inputs]\n",
    "\n",
    "    result = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }\n",
    "\n",
    "    if not is_test:\n",
    "        result[\"labels\"] = letter_to_index[example[\"answer\"]]\n",
    "\n",
    "    return result\n",
    "\n",
    "train_enc_dataset = train_dataset.map(preprocess, fn_kwargs={\"is_test\": False})\n",
    "validation_enc_dataset = validation_dataset.map(preprocess, fn_kwargs={\"is_test\": False})\n",
    "\n",
    "original_train_size = len(train_dataset)\n",
    "original_val_size = len(validation_dataset)\n",
    "\n",
    "processed_train_size = len(train_enc_dataset)\n",
    "processed_val_size = len(validation_enc_dataset)\n",
    "print(f\"Train dataset reduced by {100 * (original_train_size - processed_train_size) / original_train_size:.2f}%\")\n",
    "print(f\"Validation dataset reduced by {100 * (original_val_size - processed_val_size) / original_val_size:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT discussion\n",
    "\n",
    "The accuracy is 55%, however, it might be connected due to BERT's input limit: we can only process sequences up to 512 tokens.\n",
    "There were 2 approaches tested:\n",
    "\n",
    "**Initial approach – Removing long sequences**:\n",
    "* Tried excluding sequences >512 tokens from training.\n",
    "* Result: No improvement in accuracy, however, no reduction.\n",
    "* Besides the amount of removed data is only 7% in train and 6% in validation  = we should not expect a lot of improvement\n",
    "\n",
    "**Alternative approach – Chunking articles** (taken from https://medium.com/mim-solutions-blog/fine-tuning-bert-model-for-arbitrarily-long-texts-part-2-3211d1774dc9):\n",
    "* Split long texts into chunks ≤512 tokens with overlap of 30%.\n",
    "* Pass each chunk separately through BERT.\n",
    "* Aggregate predictions (e.g., by averaging) for final output.\n",
    "* Preserves more context and generally leads to better performance.\n",
    "Conclusion: Chunking requires to adjust the whole batch to be in the shape (num_points, num_chunks…) which adds a lot of extra code, however, no improvement in accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Falcon LLM\n",
    " * To test a pre-trained LLM (decoder-only and not fine-tuned) for the task\n",
    " * Accuracies evaluated compared to 0-, 1-, and 5-shot"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy scikit-learn\n",
    "!pip install --upgrade --no-deps transformers datasets accelerate bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for quieting the errors\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# for eval, to make it quicker run on for example [:100]\n",
    "eval_dataset = load_dataset(\"ehovy/race\",\"all\", split=\"validation\")\n",
    "dshot = load_dataset(\"ehovy/race\",\"all\", split=\"train\")\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "model_id  = \"tiiuae/falcon-rw-1b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model     = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.float16 if device==\"cuda\" else torch.float32).to(device)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "max_ctx = model.config.max_position_embeddings\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_randoms(k):\n",
    "    idxs = random.sample(range(len(dshot)), k)\n",
    "    return [dshot[i] for i in idxs]\n",
    "\n",
    "def prompt_maker(article, question, options, shots):\n",
    "    labels = [\"A\",\"B\",\"C\",\"D\"]\n",
    "    prompt = \"\"\n",
    "    for s in shots:\n",
    "        opts = [f\"{lbl}. {opt}\" for lbl,opt in zip(labels,s[\"options\"])]\n",
    "        prompt += (\n",
    "            f\"Passage:\\n{s['article']}\\n\"\n",
    "            f\"Question: {s['question']}\\n\"\n",
    "            \"Options:\\n\" + \"\\n\".join(opts) +\n",
    "            f\"\\nAnswer (only letter): {s['answer']}\\n\\n\"\n",
    "        )\n",
    "    opts = [f\"{lbl}. {opt}\" for lbl,opt in zip(labels,options)]\n",
    "    prompt += (\n",
    "        f\"Passage:\\n{article}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Options:\\n\" + \"\\n\".join(opts) +\n",
    "        \"\\nAnswer (just the letter):\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def answer_finder(text):\n",
    "    m = re.search(r\"\\b([A-Da-d])\\b\",text)\n",
    "    return m.group(1).upper() if m else None\n",
    "\n",
    "def evaluate(mode, num_shots):\n",
    "    preds, trues = [], []\n",
    "    for i, sample in enumerate(tqdm(eval_dataset)):\n",
    "        shots = get_randoms(num_shots) if num_shots else []\n",
    "        prompt = prompt_maker(sample[\"article\"],sample[\"question\"],sample[\"options\"],shots)\n",
    "\n",
    "        tokens = tokenizer.encode(prompt,add_special_tokens=False)\n",
    "        if len(tokens) > max_ctx:\n",
    "            tokens = tokens[-max_ctx:]\n",
    "\n",
    "        input_ids = torch.tensor([tokens], device=device)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                input_ids =input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens = 3,\n",
    "            )\n",
    "\n",
    "\n",
    "        gen_tokens = out[0, input_ids.shape[-1]:]\n",
    "        decoded = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # just for debugging\n",
    "        #if i < 2:\n",
    "         #  print(f\"\\n--- raw decoded [{mode},#{i}] ---\\n{decoded!r}\\n\")\n",
    "\n",
    "        pred = answer_finder(decoded) or \"\"\n",
    "        preds.append(pred)\n",
    "        trues.append(sample[\"answer\"])\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    print(f\"{mode} accuracy: {acc:.4%}\")\n",
    "    return acc"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The max token size for Falcon 1b is a bit over 2000, so 5-shot should be applicable here at least in the most cases, since in total the token size mean of one data row is about 400."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "for k in [0,1,5]:\n",
    "    name = \"0-shot\" if k==0 else f\"{k}-shot\"\n",
    "    results[name] = evaluate(name, k)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for name, a in results.items():\n",
    "    print(f\"  {name:>8}: {a:.4%}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results and conclusions Falcon 1b\n",
    "\n",
    "* Bad accuracies with no fine-tuning, however the accuracies increase when increasing the number of example prompts, compared to 0-shot.\n",
    "\n",
    "* Next steps would be fine-tuning or\n",
    "improved prompt engineering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning of Google flan t5 small\n",
    "* Parameter-efficient fine-tuning of a FLAN-T5-Small model using LoRA adapters.\n",
    "* Filters out examples exceeding 512 tokens due to a token limit on the model, we consider there to be enough data.\n",
    "* A baseline zero-shot evaluation is run before training, followed by the actual fine-tuning loop and final evaluation to compare performance gains.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "print(\"Imports successful\")\n",
    "\n",
    "raw = race\n",
    "\n",
    "print(raw) #check\n",
    "\n",
    "def preprocess(example):\n",
    "    opts = example['options']\n",
    "    input_text = (\n",
    "        f\"Passage: {example['article']} \"\n",
    "        f\"Question: {example['question']} \"\n",
    "        \"Options: \" +\n",
    "        \" | \".join([f\"{chr(65+i)}. {opt}\" for i, opt in enumerate(opts)])\n",
    "    )\n",
    "    return {\n",
    "        'input_text': input_text,\n",
    "        'target_text': example['answer']  # should be A,B,C,D\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_raw = raw['train'].map(preprocess, remove_columns=raw['train'].column_names).select(random.sample(range(87000), 20000))\n",
    "val_raw   = raw['validation'].map(preprocess, remove_columns=raw['validation'].column_names).select(random.sample(range(4700), 2000))\n",
    "print(\"Dataset loaded and preprocessed\")\n",
    "print(type(train_raw)) #type check should be <class 'datasets.arrow_dataset.Dataset'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out sequences >512 tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-small', use_fast=True)\n",
    "\n",
    "def filter_long(example):\n",
    "    return len(tokenizer(example['input_text'], truncation=False)['input_ids']) <= 512\n",
    "\n",
    "train_filtered = train_raw.filter(filter_long)\n",
    "val_filtered   = val_raw.filter(filter_long)\n",
    "print(f\"Filtered: {len(train_filtered)} train, {len(val_filtered)} val examples remain\")\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples['target_text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=4,\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    print(model_inputs)\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_filtered.map(tokenize_fn, batched=True)\n",
    "val_dataset   = val_filtered.map(tokenize_fn, batched=True)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "print(\"Tokenization complete\")\n",
    "\n",
    "#initialize model, change lora here to allow more parameters\n",
    "base_model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "print(\"Model and LoRA adapters ready\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./lora_flan_race',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=100,         # run eval every 100 steps\n",
    "    save_steps=100,         # save checkpoint every 100 steps\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "\n",
    "    metric_for_best_model='eval_loss',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    dec_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    dec_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    dec_preds = [p.strip().rstrip('.') for p in dec_preds]\n",
    "    dec_labels = [l.strip().rstrip('.') for l in dec_labels]\n",
    "    correct = sum(p == l for p, l in zip(dec_preds, dec_labels))\n",
    "    return {'accuracy': correct / len(dec_preds)}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer set up complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a random test to ensure promt quality\n",
    "random_idx = random.randint(0, len(train_dataset) - 1)\n",
    "sample = train_dataset[random_idx]\n",
    "\n",
    "input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "label_text = tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "\n",
    "input_tensor = {\n",
    "    'input_ids': sample['input_ids'].unsqueeze(0).to(model.device),\n",
    "    'attention_mask': sample['attention_mask'].unsqueeze(0).to(model.device)\n",
    "}\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **input_tensor,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    # extract only the newly generated token\n",
    "    gen_id = output_ids[0, -1].unsqueeze(0)\n",
    "    output_text = tokenizer.decode(gen_id, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== Random Sample Inference (Before Training) ===\")\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "print(f\"Target (true answer): {label_text}\")\n",
    "print(f\"Model prediction:     {output_text}\")\n",
    "print(f\"Correct? {output_text == label_text[0]}\")\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import evaluate\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "trainer.args.predict_with_generate = True\n",
    "trainer.args.generation_max_length = 2  # Need space for start token + 1 new token, does not work with only 1\n",
    "trainer.args.generation_num_beams = 1\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    #we take the second token (index 1) as that's the first generated one due to first being start token i think\n",
    "    preds = p.predictions[:, 1]\n",
    "    labels = p.label_ids[:, 0]\n",
    "    return accuracy_metric.compute(predictions=list(preds), references=list(labels))\n",
    "\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "baseline = trainer.evaluate()\n",
    "print(f\"Baseline accuracy: {baseline['eval_accuracy']:.4f}\")\n",
    "\n",
    "trainer.args.generation_max_length = 2\n",
    "trainer.args.generation_num_beams = 1\n",
    "trainer.train()\n",
    "\n",
    "final = trainer.evaluate()\n",
    "print(f\"Final accuracy:   {final['eval_accuracy']:.4f}\")\n",
    "print(f\"Baseline: {baseline['eval_accuracy']:.4f} | Final: {final['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result and Conclusion of Fine Tuned Google Flan T5 Small:\n",
    "* **Baseline: 0.4392 | Fine-Tuned: 0.5038**\n",
    "* Overall accuracy is low - with the original model only performing a small amount better than the baseline models like LSTM\n",
    "* Good improvement with the fine-tuned model compared to the small amount of changeable parameters and low computing effort achieved\n",
    "* Further training would have been appriciated, with a higher Lora rank (more changeable parameters), but due to computer limitations this was the only thing that could be done in a resonable time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# =========================================================================================="
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 3 - Extensions\n",
    "\n",
    "### ALBERT model - fine tuning and testing on another multiple-choice dataset\n",
    "\n",
    "### Distractor model - generating false answer options"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning of ALBERT-Base-v2 for Multiple-Choice QA\n",
    "* Preprocess RACE into (context, question+choices, label) format and tokenize each example.  \n",
    "* Initialize ALBERT and train on the RACE datset on multiple epochs.  \n",
    "* Evaluate on RACE and cross-evaluate on MC500 (machine comprehension of text dataset) --> accuracy jumps from ~27% to ~65% on RACE and ~27% to ~79% on MC500.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U datasets huggingface_hub fsspec"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "project_path = '/content/drive/My Drive/johanna'\n",
    "os.chdir(project_path)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForMultipleChoice, AutoTokenizer, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_alb = AutoModelForMultipleChoice.from_pretrained(\"albert-base-v2\")\n",
    "tokenizer_alb = AutoTokenizer.from_pretrained(\"albert-base-v2\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = race\n",
    "train_raw = data[\"train\"]\n",
    "val_raw   = data[\"validation\"]\n",
    "test_raw   = data[\"test\"]\n",
    "\n",
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "\n",
    "def convert_example(example):\n",
    "    answer_letter = example[\"answer\"]\n",
    "\n",
    "    if answer_letter not in letter_to_index:\n",
    "        return None\n",
    "\n",
    "    label = letter_to_index[answer_letter]\n",
    "\n",
    "    return {\n",
    "        \"context\": example[\"article\"],\n",
    "        \"question\": example[\"question\"],\n",
    "        \"choices\": example[\"options\"],\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "train_data = train_raw.map(convert_example)\n",
    "train_data = train_data.filter(lambda x: x is not None)\n",
    "\n",
    "def tokenize_mc(example):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    choices = example[\"choices\"]\n",
    "\n",
    "    # Prepare 4 pairs: (context, question + choice)\n",
    "    inputs = tokenizer_alb(\n",
    "        [context] * 4,\n",
    "        [f\"{question} {c}\" for c in choices],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "    # Each field becomes a list of 4 values\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"label\": example[\"label\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply tokenization\n",
    "tokenized_train = train_data.map(tokenize_mc)\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "val_data = val_raw.map(convert_example)\n",
    "val_data = val_data.filter(lambda x: x is not None)\n",
    "tokenized_val = val_data.map(tokenize_mc)\n",
    "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Train model\n",
    "if torch.cuda.is_available():\n",
    "    model_alb = model_alb.to(\"cuda\")\n",
    "    print(\"CUDA is available: using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available: using CPU.\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"./albert-mc-checkpoints\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=2000,\n",
    "    logging_steps=2000,\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=16,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model_alb,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer_alb,\n",
    "    compute_metrics=lambda eval_pred: {\n",
    "        \"accuracy\": accuracy_score(eval_pred[1], eval_pred[0].argmax(axis=1))\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"albert_model_saved\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_val)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing ALBERT-Base-v2 with alternative dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "\n",
    "# Converter for mc500 dataset\n",
    "def convert_mc500(example):\n",
    "    # pull out A, B, C, D in that order\n",
    "    choices = [ example[\"answer_options\"][ltr] for ltr in [\"A\",\"B\",\"C\",\"D\"] ]\n",
    "    return {\n",
    "        \"context\": example[\"story\"],\n",
    "        \"question\": example[\"question\"],\n",
    "        \"choices\": choices,\n",
    "        \"label\": letter_to_index[ example[\"answer\"] ]\n",
    "    }\n",
    "\n",
    "#Helper to predict + compute accuracy\n",
    "def predict_and_acc(model, dataset, desc):\n",
    "    trainer = Trainer(model=model, args=args)\n",
    "    print(f\"\\n→ {desc}\")\n",
    "    pred_out = trainer.predict(test_dataset=dataset)\n",
    "    preds = np.argmax(pred_out.predictions, axis=1)\n",
    "    labels = pred_out.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"Accuracy: {acc:.4f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "test_data = test_raw.map(convert_example)\n",
    "test_raw_data = test_data.filter(lambda x: x is not None)\n",
    "tokenized_test = test_data.map(tokenize_mc)\n",
    "tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load + tokenize mc500\n",
    "dataset_mc = load_dataset(\"sagnikrayc/mctest\", \"mc500\")[\"test\"]\n",
    "dataset_mc = dataset_mc.map(convert_mc500)\n",
    "dataset_mc = dataset_mc.filter(lambda x: x is not None)\n",
    "\n",
    "tokenized_mc = dataset_mc.map(tokenize_mc)\n",
    "tokenized_mc.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Final results; evaluation using the test set from RACE + also try model on MC500 dataset\n",
    "from transformers import AutoModelForMultipleChoice, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# for training\n",
    "args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=False,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "untrained_model = AutoModelForMultipleChoice.from_pretrained(\"albert-base-v2\")\n",
    "ft_model = AutoModelForMultipleChoice.from_pretrained(\"albert_model_saved\")\n",
    "\n",
    "# Predict + compute the accuracy\n",
    "predict_and_acc(untrained_model, tokenized_test, \"Original ALBERT on RACE\")\n",
    "predict_and_acc(ft_model, tokenized_test, \"Fine-tuned ALBERT on RACE\")\n",
    "predict_and_acc(untrained_model, tokenized_mc, \"Original ALBERT on MC500\")\n",
    "predict_and_acc(ft_model, tokenized_mc, \"Fine-tuned ALBERT on MC500\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion on the ALBERT model\n",
    "\n",
    "During the experiments with the ALBERT model a dramatic performance boost from fine-tuning was seen:\n",
    "\n",
    "- **Original ALBERT on RACE**: 26.51% accuracy  \n",
    "- **Fine-tuned ALBERT on RACE**: 64.65% accuracy  \n",
    "- **Original ALBERT on MC500**: 27.17% accuracy  \n",
    "- **Fine-tuned ALBERT on MC500**: 79.00% accuracy  \n",
    "\n",
    "### Takeaways\n",
    "\n",
    "1. **Fine-tuning of this model is critical**  \n",
    "   Uninitialized classification make zero-shot performance very poor, even lower than seen in the most simple regression models. Whereas fine-tuning generates results that are very good.\n",
    "\n",
    "2. **Dataset dependence**  \n",
    "   While both RACE and MC500 see the same low baseline accuracies, the fine-tuned ALBERT achieves even higher performance increases on MC500, suggensting that the fine-tuned model is capable of doing reading comprehension not only on the original dataset, but on others as well.\n",
    "\n",
    "\n",
    "Overall, our results show that a simple fine tuning of a small model like the ALBERT above can reach competitive performance on multiple-choice QA tasks, not only on the dataset it is trained on.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distractor\n",
    "* Build (input, target) pairs, filter out short targets.   \n",
    "* Fine-tune `google/flan-t5-large` for 3 epochs (bs=2, lr=5e-5) using `Seq2SeqTrainer`.  \n",
    "* Generate 5 distractor candidates with a `text2text-generation` pipeline and choose the one with lowest cosine similarity to the correct answer (using `SentenceTransformer`).  \n",
    "* Comparing average cosine-similarities: zero-shot vs fine-tuned distractors.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U datasets huggingface_hub fsspec"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# reload the dataset for this task\n",
    "\n",
    "dataset = race\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "project_path = '/content/drive/My Drive/johanna'\n",
    "os.chdir(project_path)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Make distractor input, min_target filters out short targets to help train model (before applying this the model got 0 loss and could not train)\n",
    "def make_distractor_input(ex,min_target=3):\n",
    "    inputs, targets, corr_answers = [], [], []\n",
    "    # ex[\"article\"], ex[\"questions\"], ex[\"options\"], ex[\"answers\"] are lists\n",
    "    for passage, question, opts, answer in zip(\n",
    "        ex[\"article\"], ex[\"question\"], ex[\"options\"], ex[\"answer\"]\n",
    "    ):\n",
    "        # find the correct answer\n",
    "        correct_idx = ord(answer) - ord(\"A\")\n",
    "        correct = opts[correct_idx]\n",
    "        # gather the three wrongs\n",
    "        wrongs = [opt for i,opt in enumerate(opts) if i != correct_idx]\n",
    "        # turn each wrong into its own example\n",
    "        for wrong in wrongs:\n",
    "            if len(wrong.split())>=min_target: #Filter out short targets to help train model\n",
    "              inp = (\n",
    "                  \"Generate a single, fully plausible **but definitely incorrect** answer.\\n\"\n",
    "                  \"Do NOT reuse any key phrases from the correct answer.\\n\"\n",
    "                  f\"Passage: {passage}\\n\"\n",
    "                  f\"Question: {question}\\n\"\n",
    "                  f\"Answer: {correct}\"\n",
    "              )\n",
    "              inputs.append(inp)\n",
    "              targets.append(wrong)\n",
    "              corr_answers.append(correct)\n",
    "    return {\"input_text\": inputs, \"target_text\": targets, \"correct_answer\": corr_answers}\n",
    "\n",
    "# apply to train & validation\n",
    "train = train_data.map(\n",
    "    make_distractor_input,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names\n",
    ")\n",
    "\n",
    "# Save the correct answers externally\n",
    "correct_answers_train = train[\"correct_answer\"]\n",
    "train = train.remove_columns([\"correct_answer\"])\n",
    "\n",
    "val = val_data.map(\n",
    "    make_distractor_input,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names\n",
    ")\n",
    "correct_answers_val = val[\"correct_answer\"]\n",
    "val = val.remove_columns([\"correct_answer\"])\n",
    "\n",
    "test = test_data.map(\n",
    "    make_distractor_input,\n",
    "    batched=True,\n",
    "    remove_columns=val_data.column_names\n",
    ")\n",
    "\n",
    "correct_answers_test = test[\"correct_answer\"]\n",
    "test = test.remove_columns([\"correct_answer\"])\n",
    "\n",
    "print(f\"Train examples: {len(train)}\")\n",
    "print(f\"Val examples: {len(val)}\")\n",
    "print(f\"Test examples: {len(test)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Select a smaller range, simply because of computational limits\n",
    "train_small = train.select(range(10000))\n",
    "val_small = val.select(range(1000))\n",
    "test_small = test.select(range(1000))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell below, the inputs are tokenized. In order to avoid truncation, longer examples are filtered out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "#Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "#Count input lengths\n",
    "def count_input_length(batch):\n",
    "    return {\n",
    "        \"length\": [\n",
    "            len(tokenizer.encode(text, add_special_tokens=True))\n",
    "            for text in batch[\"input_text\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "train_pre_filter = train_small.map(count_input_length, batched=True)\n",
    "val_pre_filter   = val_small.map(count_input_length, batched=True)\n",
    "test_pre_filter   = test_small.map(count_input_length, batched=True)\n",
    "\n",
    "print(\"Max train input:\", max(train_pre_filter[\"length\"]))\n",
    "print(\"Max val   input:\", max(val_pre_filter[\"length\"]))\n",
    "print(\"Max test  input:\", max(test_pre_filter[\"length\"]))\n",
    "print(f\"Before filtering: {len(train_pre_filter)} train / {len(val_pre_filter)} val /  {len(test_pre_filter)} test\")\n",
    "\n",
    "train_keep_indices = [\n",
    "    i for i, x in enumerate(train_pre_filter) if x[\"length\"] <= 512\n",
    "]\n",
    "val_keep_indices = [\n",
    "    i for i, x in enumerate(val_pre_filter) if x[\"length\"] <= 512\n",
    "]\n",
    "test_keep_indices = [\n",
    "    i for i, x in enumerate(test_pre_filter) if x[\"length\"] <= 512\n",
    "]\n",
    "\n",
    "#Filter inputs > 512\n",
    "train_filtered = train_pre_filter.select(train_keep_indices)\n",
    "val_filtered   = val_pre_filter.select(val_keep_indices)\n",
    "test_filtered   = test_pre_filter.select(test_keep_indices)\n",
    "\n",
    "correct_answers_train_filtered = [correct_answers_train[i] for i in train_keep_indices if i < len(correct_answers_train)]\n",
    "correct_answers_val_filtered = [correct_answers_val[i] for i in val_keep_indices if i < len(correct_answers_val)]\n",
    "correct_answers_test_filtered = [correct_answers_test[i] for i in test_keep_indices if i < len(correct_answers_test)]\n",
    "\n",
    "print(f\"After filtering: {len(train_filtered)} train / {len(val_filtered)} val / {len(test_filtered)} test remain\")\n",
    "\n",
    "#Count target lengths  and find true max\n",
    "def count_target_length(batch):\n",
    "    return {\n",
    "        \"target_length\": [\n",
    "            len(tokenizer.encode(t, add_special_tokens=True))\n",
    "            for t in batch[\"target_text\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "train_filtered = train_filtered.map(count_target_length, batched=True)\n",
    "val_filtered   = val_filtered.map(count_target_length, batched=True)\n",
    "test_filtered   = test_filtered.map(count_target_length, batched=True)\n",
    "max_target_len = max(train_filtered[\"target_length\"])\n",
    "max_target_len_v = max(val_filtered[\"target_length\"])\n",
    "max_target_len_test = max(test_filtered[\"target_length\"])\n",
    "max_target_len = max(max_target_len, max_target_len_v,max_target_len_test)\n",
    "print(f\"Longest target is {max_target_len} tokens\")\n",
    "\n",
    "#Final tokenization (pad inputs to 512, pad targets to max_target_len)\n",
    "def tokenize_for_model(batch):\n",
    "    # inputs: padding to 512\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=False,\n",
    "    )\n",
    "    # targets: pad to the longest target\n",
    "    labels = tokenizer(\n",
    "        batch[\"target_text\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_target_len,¨\n",
    "        truncation=False,\n",
    "    )\n",
    "        # Replace padding token (0) in labels with -100 so it's ignored in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tokenized = train_filtered.map(\n",
    "    tokenize_for_model,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"target_text\", \"length\", \"target_length\"]\n",
    ")\n",
    "val_tokenized = val_filtered.map(\n",
    "    tokenize_for_model,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"target_text\", \"length\", \"target_length\"]\n",
    ")\n",
    "\n",
    "test_tokenized = test_filtered.map(\n",
    "    tokenize_for_model,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input_text\", \"target_text\", \"length\", \"target_length\"]\n",
    ")\n",
    "\n",
    "#(Optional) switch to PyTorch tensors\n",
    "train_tokenized.set_format(type=\"torch\")\n",
    "val_tokenized.set_format(type=\"torch\")\n",
    "test_tokenized.set_format(type=\"torch\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, the model is trained. Have experimented with training arguments since a big problem was that the model first generated 0 loss. The training arguments that are chosen below are simply the first ones that worked."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#Train model\n",
    "ft_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "untrained_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "\n",
    "#Use the built-in seq2seq collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=ft_model,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "#TrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./ft-flan-t5\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=1000,\n",
    "    save_steps=3000,\n",
    "    fp16=False,                   # turn fp16 off\n",
    "    max_grad_norm=1.0,\n",
    "    label_smoothing_factor=0.1,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=ft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# manual check on a real batch\n",
    "from torch.utils.data import DataLoader\n",
    "batch = next(iter(DataLoader(train_tokenized, batch_size=2, collate_fn=data_collator)))\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    out = ft_model(**batch)\n",
    "    print(\"CHECK — loss:\", out.loss.item(),\n",
    "          \"logits min/max:\", out.logits.min().item(), out.logits.max().item())\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is a function that generates a distractor sequence. It chooses the one with lowest similarity to the correct answer, since the first models we tried tended to generate the correct answer as a distractor.\n",
    "\n",
    "Another approach that was tried for this was using a \"bad words list\", essentially banning the model from generating the correct answer. However, that led to it just switching out a word or two. For example, if correct answer = \"East Africa\", it would generate \"East London\" when the distractors were supposed to be continents. We found that that cosine_sim worked better, so we are keeping that during this implementation.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "Generates distractor sequence. Chooses the one with lowest similarity to correct answer.\n",
    "\n",
    " Args:\n",
    "        generator (pipeline): Pipeline with desired model and tokenizer.\n",
    "        prompt (String): The prompt, includes article, question, and correct answer.\n",
    "        correct (String): The correct answer, used for cosine similarity measure.\n",
    "        num_samples (int): The amount of samples to generate\n",
    "\n",
    "    Returns:\n",
    "        cands (String): The best candidate distractor, i.e. the one with lowest cosine similarity to correct answer\n",
    "        sims: The cosine similarity of the returned distractor\n",
    "'''\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "def generate_distractor(generator, prompt, correct, num_samples=5):\n",
    "    # Generate text\n",
    "    outputs = generator(\n",
    "        prompt['input_text'],\n",
    "        max_length=64,\n",
    "        temperature=0.9, #Allow picking less likely words to make model more \"creative\" (to avoid generated sequences being too similar)\n",
    "        top_p=0.9, #same as above, let model consider more words/tokens\n",
    "        num_return_sequences=num_samples,\n",
    "        no_repeat_ngram_size=2, #avoid repeating\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract generated texts\n",
    "    cands = [out['generated_text'] for out in outputs]\n",
    "\n",
    "\n",
    "    # embed & pick least-similar\n",
    "    corr_emb  = embed_model.encode([correct], convert_to_tensor=True)\n",
    "    cand_embs = embed_model.encode(cands,  convert_to_tensor=True)\n",
    "    sims      = cosine_similarity(cand_embs, corr_emb).flatten().tolist()\n",
    "    best_idx  = min(range(len(sims)), key=lambda i: sims[i])\n",
    "    return cands[best_idx], sims[best_idx]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "#Evaluate both trained and untrained model on test set\n",
    "\n",
    "ft_model = T5ForConditionalGeneration.from_pretrained(\"flan_final_save\").to(device)\n",
    "zero_sims, ft_sims, zero_dist, ft_dist = [], [], [], []\n",
    "i=0\n",
    "#Define pipelines\n",
    "zero_generator = pipeline(\"text2text-generation\", model=untrained_model, tokenizer=tokenizer)\n",
    "ft_generator = pipeline(\"text2text-generation\", model=ft_model, tokenizer=tokenizer)\n",
    "\n",
    "for prompt in test_filtered: #Generate distractors for the test samples\n",
    "    correct = correct_answers_test_filtered[i]\n",
    "\n",
    "    #generate distractor for original model\n",
    "    zs, zs_score = generate_distractor(zero_generator, prompt, correct)\n",
    "\n",
    "    zero_sims.append(zs_score)\n",
    "    zero_dist.append(zs)\n",
    "\n",
    "    # fine-tuned model, generate distractor\n",
    "    ft,ft_score = generate_distractor(ft_generator,prompt, correct)\n",
    "    ft_sims.append(ft_score)\n",
    "    ft_dist.append(ft)\n",
    "    i += 1\n",
    "\n",
    "#Report\n",
    "print(f\"Avg cosine-sim to correct answer:\")\n",
    "print(f\"Zero-shot:  {sum(zero_sims)/len(zero_sims):.4f}\")\n",
    "print(f\"Fine-tuned: {sum(ft_sims)/len(ft_sims):.4f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comments on distractor result\n",
    "After training, we can see that the model on average generates distractors with somewhat lower similarity to the correct answer. This is a good sign, as we do not want our distractors to be close to the correct answer. However, it is by no means a complete measure of the model's performance. Furthermore, we do not know what an optimal similarity measure is. If the distractors are too unsimilar to the correct answer, they will be very easy to spot and make the question easier than intended. \\\\\n",
    "\n",
    "Considering the long input passages, this is quite a complex task, and the most correct evaluation would be to manually look at them. While we can't go through all of them, we have manually gone through the first 5 questions and the three generated distractors for all of them and grouped them into 4 categories:\n",
    "- Correct (a good distractor)\n",
    "- Synonym to correct answer\n",
    "- Semantically strange or grammatically incorrect\n",
    "- Incorrect (Not a wrong answer. Eg. if the question is \"which is true?\", the generated distractor is also true)\n",
    "\n",
    "### Results\n",
    "\n",
    "**For untrained:**\n",
    " 6 correct, 6 synonyms, 2 incorrect, 2 strange \\\\\n",
    "**For fine-tuned:**\n",
    "13 correct, 2 incorrect\n",
    "\n",
    "While the above is a very small sample size, it definitely seems that the model has improved by training. What should be noted is that \"correct\" does not mean that the generated distractor is as good as the original ones. Instead, the generated distractors sometimes makes the question easier than it would have been with the original distractors.\n",
    "\n",
    "-----\n",
    "Evaluations made  \\\\\n",
    "1: UT: 3 synonyms to correct answer. FT: 3 correct\n",
    "\n",
    "2:\n",
    "UT: 2 correct, 1 incorrect.\n",
    "FT: 1 correct, 2 incorrect.\n",
    "\n",
    "3:\n",
    "UT: 3 correct.\n",
    "FT: 3 correct.\n",
    "\n",
    "4:\n",
    "UT: 1 synonym to original answer, 1 semantically strange, 1 gramatically incomplete/strange.\n",
    "FT: 3 correct.\n",
    "\n",
    "5:\n",
    "UT: 1 correct, 2 synonyms to correct answer.\n",
    "FT: 3 correct."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#Printing 50 examples for manual evaluation\n",
    "import textwrap\n",
    "def print_wrapped(label, text, width=160):\n",
    "    wrapped = textwrap.fill(text, width=width)\n",
    "    print(f\"{label}:\\n{wrapped}\\n\")\n",
    "\n",
    "#Look at 50 examples with three predictions each\n",
    "for i in range(0,150,3):\n",
    "    print(f\"--- Example {i/3} ---\")\n",
    "    print_wrapped(\"Input text\", test_filtered[i][\"input_text\"])\n",
    "    print(\"Distractors untrained:\", zero_dist[i:i+3])\n",
    "    print(\"Distractors fine-tuned:\", ft_dist[i:i+3])\n",
    "    print(\"Target distractors:\", test_filtered[i:i+3][\"target_text\"])\n",
    "    print()\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test with high end LLM - Google Gemeni 2.0-flash\n",
    "* Comparison of zero, one and five shot performance\n",
    "* Comparison on performance on the two different dataset splits, high school vs middle school"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test 0 vs 1 vs 5 shot promting\n",
    "* 100 tests on 0, 1 and 5 shot (300 prompts in total)\n",
    "* The split was 72 high vs 28 middle which is representable for the dataset as a whole"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "# retrived from documenteation from google api\n",
    "#API KEY removed fro securirt\n",
    "def get_answer(api_key: str, prompt: str, model: str = \"gemini-2.0-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Uses the new google-genai SDK to get a plain-text answer.\n",
    "    \"\"\"\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset(\"ehovy/race\",'all', split=None)\n",
    "dataset_train = dataset['train']\n",
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def build_prompt(article, shots = 0):\n",
    "    \"\"\"\n",
    "    Build the prompt for the model.\n",
    "    \"\"\"\n",
    "    prompt = \"Answer the question with only one capillized letter (A, B, C or D).\\n\" #extra instruct\n",
    "    for i in range(shots):\n",
    "        support_article = random.choice(dataset_train)\n",
    "        # re‐draw if it’s the same as the main article\n",
    "        while support_article['example_id'] == article['example_id']:\n",
    "            support_article = random.choice(dataset_train)\n",
    "        prompt += f\"Article: {support_article['article']}\\nQuestion: {support_article['question']}\\n\"\n",
    "        for idx, option in enumerate(support_article['options']):\n",
    "            label = chr(ord('A') + idx)\n",
    "            prompt += f\"{label}. {option}\\n\"\n",
    "        prompt += f\"Answer: {support_article['answer']}\\n\\n\"\n",
    "\n",
    "    prompt += f\"Article: {article['article']}\\nQuestion: {article['question']}\\n\"\n",
    "    for idx, option in enumerate(article['options']):\n",
    "            label = chr(ord('A') + idx)\n",
    "            prompt += f\"{label}. {option}\\n\"\n",
    "    prompt += \"Answer: \"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = random.randint(0, 80000)\n",
    "print(\"Random index:\", row)\n",
    "print(build_prompt(dataset_train[row], 0))\n",
    "print(build_prompt(dataset_train[row], 1))\n",
    "print(build_prompt(dataset_train[row], 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a single example\n",
    "article = dataset_train[row]  # row is already defined randomly\n",
    "prompt = build_prompt(article, shots=0)\n",
    "response = get_answer(API_KEY, prompt).strip()\n",
    "print(prompt)\n",
    "print(\"Input length:\", len(prompt))\n",
    "print(\"Model response:\", response)\n",
    "print(\"Correct answer:\", article['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, time, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.api_core.exceptions import GoogleAPICallError\n",
    "\n",
    "CHECKPOINT_FILE = \"qa_results.csv\"\n",
    "\n",
    "def safe_get_answer(api_key: str, prompt: str, model: str = \"gemini-2.0-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Calls get_answer() and on any HTTP error (429 or any 5xx),\n",
    "    backs off with jitter & retries indefinitely.\n",
    "    \"\"\"\n",
    "    backoff, max_backoff = 30, 300\n",
    "    while True:\n",
    "        try:\n",
    "            return get_answer(api_key, prompt, model).strip()\n",
    "        except GoogleAPICallError as e:\n",
    "            # figure out HTTP code if available\n",
    "            code = getattr(e, \"status_code\", None) or getattr(e, \"code\", None)\n",
    "            # treat any 5xx or 429 as retryable so it goes again to eliminate crash if timeout\n",
    "            if code == 429 or (isinstance(code, int) and 500 <= code < 600):\n",
    "                wait = min(backoff, max_backoff)\n",
    "                jitter = wait * 0.1\n",
    "                wait = wait + random.uniform(-jitter, jitter)\n",
    "                print(f\"[HTTP {code}] retryable error, sleeping {int(wait)}s…\")\n",
    "                time.sleep(wait)\n",
    "                backoff = min(backoff * 2, max_backoff)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "def load_checkpoint():\n",
    "    if not os.path.exists(CHECKPOINT_FILE):\n",
    "        return set()\n",
    "    done = set()\n",
    "    with open(CHECKPOINT_FILE, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        for row in csv.DictReader(f):\n",
    "            done.add((int(row[\"shots\"]), int(row[\"idx\"])))\n",
    "    return done\n",
    "\n",
    "def append_result(shots: int, idx: int, pred: str, correct: bool):\n",
    "    file_existed = os.path.exists(CHECKPOINT_FILE)\n",
    "    with open(CHECKPOINT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"shots\",\"idx\",\"prediction\",\"correct\"])\n",
    "        if not file_existed:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\n",
    "            \"shots\": shots,\n",
    "            \"idx\": idx,\n",
    "            \"prediction\": pred,\n",
    "            \"correct\": correct\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = random.sample(range(len(dataset_train)), 100)\n",
    "shots_list = [0, 1, 5]\n",
    "done = load_checkpoint()\n",
    "\n",
    "print(f\"Will process {len(rows)} rows × {len(shots_list)} shot-settings, skipping {len(done)} done cells.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE WORKER LOOP\n",
    "for shots in shots_list:\n",
    "    for idx in tqdm(rows, desc=f\"{shots}-shot\"):\n",
    "        if (shots, idx) in done:\n",
    "            continue  # already logged → skip\n",
    "\n",
    "        article = dataset_train[idx]\n",
    "        prompt  = build_prompt(article, shots)\n",
    "\n",
    "        pred = safe_get_answer(API_KEY, prompt)\n",
    "        time.sleep(20)  # pace: one request per 20s so not to timeout\n",
    "\n",
    "        is_correct = (pred == article[\"answer\"])\n",
    "        append_result(shots, idx, pred, is_correct)\n",
    "        print(f'{idx} is complete')\n",
    "        done.add((shots, idx))\n",
    "\n",
    "print(\"Run complete — rerun this cell until all (shots, idx) pairs are done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(CHECKPOINT_FILE)\n",
    "\n",
    "accuracy = df.groupby(\"shots\")[\"correct\"].mean()\n",
    "print(\"Final accuracies:\\n\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result Shot Comparison\n",
    "* 0 -    0.90\n",
    "* 1 -    0.84\n",
    "* 5 -    0.92"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing comparing high school data vs middle school data\n",
    "* 100 of each, run in 0, 1, 5 shot (600 prompts total)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, time, random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.api_core.exceptions import GoogleAPICallError\n",
    "\n",
    "CHECKPOINT_FILE = \"middle_vs_high.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "high_idxs = [i for i, ex in enumerate(dataset_train) if ex['example_id'].startswith(\"high\")]\n",
    "middle_idxs = [i for i, ex in enumerate(dataset_train) if ex['example_id'].startswith(\"middle\")]\n",
    "rows = random.sample(high_idxs, 100) + random.sample(middle_idxs, 100)\n",
    "\n",
    "shots_list = [0, 1, 5]\n",
    "done = load_checkpoint()\n",
    "\n",
    "print(f\"Will process {len(rows)} rows × {len(shots_list)} shot-settings, skipping {len(done)} done cells.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE WORKER LOOP\n",
    "for shots in shots_list:\n",
    "    for idx in tqdm(rows, desc=f\"{shots}-shot\"):\n",
    "        if (shots, idx) in done:\n",
    "            continue  # already logged → skip\n",
    "\n",
    "        article = dataset_train[idx]\n",
    "        prompt  = build_prompt(article, shots)\n",
    "\n",
    "        pred = safe_get_answer(API_KEY, prompt)\n",
    "        time.sleep(15)  # pace: one request per 15s so not to timeout\n",
    "\n",
    "        is_correct = (pred == article[\"answer\"])\n",
    "        append_result(shots, idx, pred, is_correct)\n",
    "        print(f'{idx} is complete')\n",
    "        done.add((shots, idx))\n",
    "\n",
    "print(\"Run complete — rerun this cell until all (shots, idx) pairs are done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(CHECKPOINT_FILE)\n",
    "\n",
    "df['level'] = df['idx'].apply(lambda i: 'high' if rows.index(i) < 100 else 'middle')\n",
    "acc_by_shot_and_level = df.groupby(['shots', 'level'])['correct'].mean().unstack()\n",
    "\n",
    "print(\"Accuracy by shot and level:\\n\", acc_by_shot_and_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusions and Results of the complex LLM\n",
    "\n",
    "* Very good accuracy overall, as expected by such a huge model (**around 90%**).\n",
    "* Intresting to see that one-shot prompting performs worse than zero-shot. One‐shot prompting can sometimes inadvertently bias the model toward that specific format or domain, causing it to overfit to the example rather than generalizing your actual question, while zero‐shot forces the model to rely solely on its pre‐trained knowledge. This can almost be confimed by the five-shot performing slighlty better than zero-shot.\n",
    "* As expected, the more simple middel school questions were easier (**88% vs 96%**). This margin was bigger than expected. It shows that the level of the question is very important when measuring accuracy and must be taken into account when performing comparisons on different datasets."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
